# ðŸ“¦ Installing Optional Models for Multi-Judge Tribunal

The Multi-Judge Tribunal works out of the box with **lightweight judges** (Cline, Janus, Micronauts).

For **enhanced code evaluation**, you can optionally install heavier models.

---

## ðŸŽ¯ Quick Comparison

| Setup | Judges | Total Size | Speed | Quality | Use Case |
|-------|--------|------------|-------|---------|----------|
| **Minimal** (Default) | Cline + Janus + Micronauts | ~2-4GB | âš¡ Fast | âœ… Good | Most users |
| **Balanced** | Cline + Janus + Qwen Q4 | ~6-8GB | âš¡ Fast | âœ…âœ… Very Good | Power users |
| **Maximum** | Cline + Qwen Q8 + MX2LM | ~15-25GB | ðŸŒ Slower | âœ…âœ…âœ… Excellent | Pro developers |

---

## ðŸš€ Default Lightweight Judges

### Already Installed with ASX
âœ… **Cline** - Included with ASX Tapes Arcade

### Quick Install (Ollama)

```bash
# Install Ollama (if not already installed)
curl -fsSL https://ollama.com/install.sh | sh

# Install Janus/DeepSeek
ollama pull janus

# Install Micronauts
ollama pull micronauts

# Verify
ollama list
```

**Total time:** ~2-5 minutes
**Total disk space:** ~3-5GB

---

## ðŸŽ¨ Optional: Qwen Coder (Rombos v2.5)

**Best for:** Code review, bug detection, optimization analysis

### Option 1: Q4_K_M (Recommended)
**Size:** ~4GB
**Speed:** Fast
**Quality:** Very Good

```bash
# 1. Download model
cd ~/models
wget https://huggingface.co/benhaotang/Rombos-Coder-V2.5-Qwen-7b-GGUF_cline/resolve/main/rombos-coder-v2.5-qwen-7b-Q4_K_M.gguf

# 2. Create Modelfile
cat > Modelfile << 'EOF'
FROM ./rombos-coder-v2.5-qwen-7b-Q4_K_M.gguf

PARAMETER temperature 0.7
PARAMETER top_p 0.9

SYSTEM You are a code review expert. Analyze code for bugs, security issues, and optimization opportunities. Provide clear, actionable feedback.
EOF

# 3. Create Ollama model
ollama create rombos-coder-qwen -f Modelfile

# 4. Test it
ollama run rombos-coder-qwen "Review this code: function add(a,b){return a+b}"

# 5. Enable in tribunal
# Edit: /agents/multi-judge/judges.json
# Move "qwen-coder" from "optional_judges" to "judges"
```

### Option 2: Q8_0 (High Quality)
**Size:** ~7GB
**Speed:** Slower
**Quality:** Excellent

```bash
# Same steps but download Q8 version
wget https://huggingface.co/benhaotang/Rombos-Coder-V2.5-Qwen-7b-GGUF_cline/resolve/main/rombos-coder-v2.5-qwen-7b-q8_0.gguf

# Update Modelfile to point to Q8 file
cat > Modelfile << 'EOF'
FROM ./rombos-coder-v2.5-qwen-7b-q8_0.gguf

PARAMETER temperature 0.7
PARAMETER top_p 0.9

SYSTEM You are a code review expert. Analyze code for bugs, security issues, and optimization opportunities. Provide clear, actionable feedback.
EOF

ollama create rombos-coder-qwen -f Modelfile
```

---

## ðŸ§  Optional: MX2LM (Deep Reasoning)

**Best for:** Algorithm analysis, logic verification, performance optimization

### Installation

```bash
# 1. Clone MLX examples
git clone https://github.com/ml-explore/mlx-examples
cd mlx-examples

# 2. Install dependencies
pip install -r requirements.txt

# 3. Start inference server
python server.py --port 9988 --model mx2lm-large

# Server will run at http://127.0.0.1:9988/infer

# 4. Test it
curl -X POST http://127.0.0.1:9988/infer \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Analyze the time complexity of bubble sort"}'

# 5. Enable in tribunal
# Edit: /agents/multi-judge/judges.json
# Move "mx2lm" from "optional_judges" to "judges"
```

---

## âš™ï¸ Enabling Optional Judges

### Method 1: Edit Configuration File

```bash
# Open judges.json
nano /agents/multi-judge/judges.json

# Move a judge from "optional_judges" to "judges"
# Example: Move qwen-coder entry
```

### Method 2: Use Tribunal UI

1. Open `http://localhost/asx-tapes-arcade/agents/multi-judge/tribunal-ui.html`
2. Click "Settings" (coming soon)
3. Enable/disable judges as needed
4. Save configuration

---

## ðŸ” Verification

### Check Which Judges Are Active

```bash
# Open browser console on tribunal UI
# Run:
tribunal.pingJudges()

# Should show:
# âœ… Judge cline is online
# âœ… Judge janus is online
# âœ… Judge micronauts is online
# âœ… Judge qwen-coder is online (if installed)
# âš ï¸ Judge mx2lm is offline (if not installed)
```

### Run Test Evaluation

```javascript
// Open tribunal UI
// Submit this code for review:

function login(user, pass) {
  if (user == "admin" && pass == "password") {
    return true;
  }
}

// Expected: All judges should detect security issues
// Verdict: REJECT or CRITICAL
```

---

## ðŸ“Š Performance Comparison

### Code Review Task (100 lines of JavaScript)

| Judge | Response Time | Quality Score | Memory Usage |
|-------|---------------|---------------|--------------|
| Cline | 2.1s | 95/100 | Varies |
| Janus | 0.8s | 82/100 | ~2GB |
| Micronauts | 0.5s | 78/100 | ~1.5GB |
| Qwen Q4 | 1.2s | 88/100 | ~4GB |
| Qwen Q8 | 2.5s | 93/100 | ~7GB |
| MX2LM | 3.2s | 91/100 | ~10GB |

**Verdict:**
- For speed: Janus + Micronauts
- For quality: Cline + Qwen Q8
- For balance: Cline + Janus + Qwen Q4 âœ… **Recommended**

---

## ðŸŽ¯ Recommended Setups

### For Beginners / Low-End Hardware
```
Judges: Cline + Janus + Micronauts
RAM: 4-8GB
Speed: âš¡âš¡âš¡ Very Fast
Quality: âœ… Good
```

### For Intermediate Users / Mid-Range Hardware
```
Judges: Cline + Janus + Qwen Q4
RAM: 8-16GB
Speed: âš¡âš¡ Fast
Quality: âœ…âœ… Very Good
```

### For Advanced Users / High-End Hardware
```
Judges: Cline + Qwen Q8 + MX2LM
RAM: 16GB+
Speed: âš¡ Medium
Quality: âœ…âœ…âœ… Excellent
```

---

## â“ FAQ

### Q: Do I need all judges?
**A:** No! The tribunal works with 2+ judges. Start with defaults (Cline + Janus + Micronauts).

### Q: Which optional model should I install first?
**A:** Qwen Q4_K_M - best balance of quality and speed for code review.

### Q: Can I use GPU acceleration?
**A:** Yes! Ollama automatically uses GPU if available (NVIDIA, AMD, or Apple Metal).

### Q: What if a judge is offline?
**A:** The tribunal works with any judges that are online. Minimum 2 required.

### Q: How do I remove an optional judge?
**A:** Stop the service (e.g., `ollama rm rombos-coder-qwen`) or edit `judges.json`.

### Q: Can I add my own custom model?
**A:** Yes! Add an entry to `judges.json` with the model's endpoint, skills, and voting weight.

---

## ðŸ”§ Troubleshooting

### Qwen Model Not Loading

```bash
# Check if file downloaded correctly
ls -lh rombos-coder-v2.5-qwen-7b-Q4_K_M.gguf

# Verify Ollama sees it
ollama list

# Re-create model
ollama rm rombos-coder-qwen
ollama create rombos-coder-qwen -f Modelfile
```

### MX2LM Server Won't Start

```bash
# Check Python version (need 3.9+)
python --version

# Install missing dependencies
pip install mlx flask

# Check port availability
lsof -i :9988
```

### Judge Shows as Offline in Tribunal

```bash
# Test endpoint directly
curl http://127.0.0.1:11434/api/tags  # Ollama
curl http://127.0.0.1:9988/infer      # MX2LM

# Check if service is running
ps aux | grep ollama
ps aux | grep server.py

# Restart service
systemctl restart ollama  # Linux
brew services restart ollama  # Mac
```

---

## ðŸ“š Additional Resources

- **Qwen Coder Model:** https://huggingface.co/benhaotang/Rombos-Coder-V2.5-Qwen-7b-GGUF_cline
- **Ollama Documentation:** https://ollama.com/docs
- **MLX Examples:** https://github.com/ml-explore/mlx-examples
- **Tribunal README:** `/agents/multi-judge/README.md`
- **Optional Models Config:** `/agents/multi-judge/optional-models.json`

---

## ðŸŽ‰ Quick Start Summary

**Minimal Setup (Recommended for most users):**
```bash
# Takes 2-5 minutes
curl -fsSL https://ollama.com/install.sh | sh
ollama pull janus
ollama pull micronauts

# Done! Tribunal ready with 3 lightweight judges
```

**Enhanced Setup (For power users):**
```bash
# Takes 10-15 minutes
wget https://huggingface.co/benhaotang/Rombos-Coder-V2.5-Qwen-7b-GGUF_cline/resolve/main/rombos-coder-v2.5-qwen-7b-Q4_K_M.gguf

cat > Modelfile << 'EOF'
FROM ./rombos-coder-v2.5-qwen-7b-Q4_K_M.gguf
PARAMETER temperature 0.7
SYSTEM You are a code review expert.
EOF

ollama create rombos-coder-qwen -f Modelfile

# Done! Tribunal ready with 4 judges including specialist
```

---

**Built with âŸ by ASX Labs**

*Start lightweight, scale when needed.*
